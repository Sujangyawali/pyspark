{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee48c4d8-4f99-432e-a709-67a348ce5190",
   "metadata": {},
   "source": [
    "They’re just functions that operate on the data, record by record.\n",
    "By default, these functions are registered as temporary functions to be used in that specific\n",
    "SparkSession or Context.\n",
    "we need to register them with Spark so that we can use them on all of our worker machines. Spark will serialize the function on the driver and transfer it over the network to all executor processes. This happens regardless of\n",
    "language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0b23f0-7753-4a75-921f-43ed8f47ae06",
   "metadata": {},
   "source": [
    "In Apache Spark,a user-defined function (UDF), the behavior and performance can vary depending on the language in which the function is written.Two key points about using UDFs in Spark, particularly when they are written in Scala or Java:\n",
    "1. Running within the Java Virtual Machine (JVM):\n",
    "   - Scala and Java UDFs run directly within the JVM, which is the same environment where Spark itself runs. This means there is no additional overhead of transferring data between different runtime environments (e.g., between the JVM and a Python interpreter).\n",
    "   - Since Spark is written in Scala and runs on the JVM, UDFs written in Scala or Java integrate seamlessly with Spark's internal operations. This tight integration generally results in better performance compared to UDFs written in languages like Python, which require data to be serialized and transferred between the JVM and the Python interpreter.\n",
    "2. Lack of Code Generation Optimization:\n",
    "   - Spark has a powerful optimization feature called *code generation* for its built-in functions. Code generation allows Spark to dynamically generate optimized bytecode at runtime for specific operations, which can significantly improve performance.\n",
    "   - However, when we use a custom UDF (written in Scala, Java, or any other language), Spark cannot apply this code generation optimization to your UDF. This is because the UDF is treated as a \"black box\" — Spark does not have insight into the logic inside the UDF and therefore cannot optimize it in the same way it optimizes built-in functions.\n",
    "   - As a result, UDFs typically incur a performance penalty compared to using Spark's built-in functions, even when written in Scala or Java.\n",
    "\n",
    "## Summary:\n",
    "- Advantage: Scala/Java UDFs run efficiently within the JVM, avoiding the overhead of inter-process communication (unlike Python UDFs).\n",
    "\n",
    "- Disadvantage: UDFs cannot leverage Spark's code generation optimizations, which can lead to slower performance compared to built-in functions.\n",
    "\n",
    "## Best Practices:\n",
    "- Whenever possible, use Spark's built-in functions instead of UDFs, as they are highly optimized.\n",
    "- If must use scenario a UDF, prefer Scala or Java over Python for better performance.\n",
    "- Consider rewriting UDF logic using Spark's DataFrame API or SQL expressions to take advantage of Spark's optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9006dc-b46b-4fa2-b2b9-166505677fac",
   "metadata": {},
   "source": [
    "# Python-JVM Communication in Apache Spark\n",
    "\n",
    "In Apache Spark, **Python interpreters need to communicate with the JVM** because Spark’s core engine (written in Scala/Java) runs on the JVM, while PySpark (Spark’s Python API) runs in a separate Python process. This communication is required to coordinate tasks, serialize data, and execute operations across distributed clusters. Let’s break this down with examples and technical details.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why Python and JVM Need to Communicate**\n",
    "\n",
    "1. **Architecture of PySpark**:\n",
    "   - **Spark Core** (written in Scala/Java) runs on the JVM and manages distributed data processing.\n",
    "   - **PySpark** (Python API) acts as a \"bridge\" between Python code and the JVM-based Spark engine.\n",
    "   - Python itself is not JVM-based, so data and tasks must be exchanged between the two processes.\n",
    "\n",
    "2. **Example Scenario**:\n",
    "   ```python\n",
    "   # Python code using PySpark\n",
    "   from pyspark.sql import SparkSession\n",
    "   spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "   df = spark.read.csv(\"data.csv\")\n",
    "   df.show()  # This triggers communication with the JVM\n",
    "\n",
    "\n",
    "## How Python-JVM Communication Works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "083a6f35-14e0-4417-a918-fc899640890b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m power3_udf \u001b[38;5;241m=\u001b[39m udf(power3, DoubleType())\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Create a DataFrame\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m udfExampleDF \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mrange(\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mtoDF(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Apply the UDF\u001b[39;00m\n\u001b[1;32m     15\u001b[0m resultDF \u001b[38;5;241m=\u001b[39m udfExampleDF\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_cubed\u001b[39m\u001b[38;5;124m\"\u001b[39m, power3_udf(udfExampleDF[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Define a Python function\n",
    "def power3(double_value):\n",
    "    return double_value ** 3\n",
    "\n",
    "# Register it as a UDF\n",
    "power3_udf = udf(power3, DoubleType())\n",
    "\n",
    "# Create a DataFrame\n",
    "udfExampleDF = spark.range(5).toDF(\"num\")\n",
    "\n",
    "# Apply the UDF\n",
    "resultDF = udfExampleDF.withColumn(\"num_cubed\", power3_udf(udfExampleDF[\"num\"]))\n",
    "\n",
    "# Show the result\n",
    "resultDF.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
