{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD\n",
    "An RDD (Resilient Distributed Dataset) is a fundamental data structure in Apache Spark that represents a distributed collection of data spread across multiple nodes in a cluster. RDDs are designed for fault tolerance, parallel processing, and efficient data manipulation.RDDs have a specialized structure with unique properties designed for distributed data processing.\n",
    "\n",
    "### Structure of RDD:\n",
    "An RDD is composed of the following key components:\n",
    "1.  **Elements**: The actual data stored in the RDD, which can be of any type.\n",
    "2.  **Partitions**: RDDs are split into partitions, which are chunks of data distributed across nodes in the Spark cluster. Each partition is a subset of the dataset, allowing parallel operations on data.Partitions can be based on the source of the data (like file splits in HDFS) or user-defined partitioning strategies.Partitioning enables RDD operations to be parallelized, increasing efficiency and performance by spreading computation across multiple nodes.\n",
    "3.  **Dependencies**: Dependencies define the relationships between RDDs when transformations are applied. Spark has two types of dependencies:\n",
    "    - **Narrow Dependency**: Each partition of the child RDD depends on a single partition of the parent RDD (e.g., map, filter). This minimizes data shuffling and allows easy parallel processing.\n",
    "    - **Wide Dependency**: Each partition of the child RDD depends on multiple partitions of the parent RDD (e.g., groupByKey, reduceByKey), requiring data shuffling across nodes.\n",
    "By managing dependencies, Spark can optimize execution, deciding when data needs to be moved or shuffled between nodes and minimizing unnecessary data transfers.\n",
    "4.  **Lineage (Directed Acyclic Graph - DAG)**: RDDs can be cached in memory for faster access\n",
    "    Lineage is a DAG that records the sequence of transformations applied to create an RDD. Rather than storing intermediate results, Spark builds this graph to track transformations and recompute data when necessary.\n",
    "    Each transformation on an RDD creates a new RDD, preserving the history of transformations in the form of a DAG. Spark uses this lineage graph to reconstruct lost partitions, ensuring fault tolerance.\n",
    "    This DAG-based lineage allows Spark to recover lost data by reapplying transformations on the original data source or earlier RDDs, providing resilience to failures.\n",
    "\n",
    "5. **Persistence and Caching**: RDDs can be cached (stored in memory) or persisted to disk, allowing reuse without recomputation.Caching RDDs helps avoid recomputation of the entire lineage when RDDs are used repeatedly in an application.Persistence is especially beneficial for iterative computations, where RDDs are used multiple times (like in machine learning algorithms).\n",
    "\n",
    "6. **Fault Tolerance and Checkpointing**:Spark automatically handles faults by recomputing the lineage of an RDD to recover lost partitions. Additionally, checkpointing stores RDD data at a particular point in the lineage, discarding the lineage information from that point backward.\n",
    "If a partition is lost, Spark replays the transformations in the lineage graph from a previous RDD (or a checkpoint) to regenerate it.\n",
    "This system enables Spark to handle failures without data loss, balancing between recomputation (through lineage) and storage costs (through checkpointing).\n",
    "\n",
    "### RDDs are made up of 4 parts:\n",
    "\n",
    "- Partitions: Atomic pieces of the dataset. One or many per compute node.\n",
    "- Dependencies: Models relationship between this RDD and its partitions with the RDD(s) it was derived from. (Note that the dependencies maybe modeled per partition as shown below).\n",
    "- A function for computing the dataset based on its parent RDDs.\n",
    "- Metadata about it partitioning scheme and data placement.\n",
    "\n",
    "### Status of RDDs before and after action program:\n",
    "- Before action program: RDDs are in a transient state. They are not yet materialized,holds a reference (lineage(dag) and data source).\n",
    "- After action program: RDD materializes actual data in memory if an action like collect or count is called.RDDs are materialized. They are stored in memory or on disk.\n",
    "\n",
    "### RDDs are used in the following ways:\n",
    "- **Transformation**: RDDs are transformed using methods like map, filter, reduce, groupByKey,\n",
    "- **Action**: RDDs are used to perform actions like collect, count, saveAsTextFile\n",
    "- **Persistence**: RDDs can be persisted to memory or disk for faster access.\n",
    "- **Checkpointing**: RDDs can be checkpointed to disk to ensure fault tolerance.\n",
    "\n",
    "### RDDs are used in the following scenarios:\n",
    "- **Data processing**: RDDs are used to process large datasets in parallel.\n",
    "- **Machine learning**: RDDs are used to train machine learning models on large datasets.\n",
    "- **Data analysis**: RDDs are used to analyze large datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark cluster\n",
    "A Spark cluster is a collection of computers (nodes) that work together to run Spark applications. It allows Spark to process large-scale data efficiently by distributing workloads across multiple machines. Hereâ€™s a detailed overview of the components and architecture of a Spark cluster:\n",
    "\n",
    "### Components of a Spark Cluster\n",
    "1. **Master Node**:\n",
    "    The master node (also known as the  cluster manager) is responsible for managing the cluster, scheduling tasks, and resource allocation.\n",
    "    It tracks the status of the worker nodes, maintains metadata about the RDDs, and coordinates the execution of applications.\n",
    "    The master can run in different modes, including standalone mode, on YARN (Hadoop Yet Another Resource Negotiator), or Mesos.\n",
    "\n",
    "2.  **Worker Nodes**:\n",
    "    Worker nodes are the machines that execute the tasks assigned by the master node. Each worker node can run multiple tasks in parallel based on its resources (CPU, memory).\n",
    "    Each worker node hosts one or more executor processes, which are responsible for executing the tasks and storing data for RDDs in memory or disk.\n",
    "    Worker nodes communicate with the master node to report their status and receive tasks.\n",
    "\n",
    "3.  **Executor**:\n",
    "    Executors are processes running on worker nodes that are responsible for executing the individual tasks assigned by the master.\n",
    "    They manage the storage of data for RDDs (in memory or on disk) and the execution of computations.\n",
    "    Each executor can run multiple tasks simultaneously, depending on the resources allocated.\n",
    "\n",
    "4.  **Driver Program**:\n",
    "    The driver program is the main application that orchestrates the execution of the Spark application. It runs the main function and creates the SparkContext[SparkContext is the entry point for any Spark functionality. It represents the connection to a Spark cluster and can be used to create RDDs (Resilient Distributed Datasets), accumulators, and broadcast variables. SparkContext also coordinates the execution of tasks.], which connects to the cluster manager.It communicates with the master to schedule tasks and track their progress.The driver maintains the lineage of RDDs and manages data processing workflows.\n",
    "\n",
    "5. **Cluster Manager**:\n",
    "    The cluster manager is a service that manages resources across the cluster.(Spark supports various cluster managers like Apache Mesos, Hadoop YARN, and standalone cluster manager.) It can be a standalone Spark cluster manager, Hadoop YARN, or Apache Mesos.The cluster manager handles resource allocation, scheduling, and monitoring of the cluster, deciding how many resources to allocate to each application.\n",
    "\n",
    "### Spark Cluster Architecture\n",
    "The architecture of a Spark cluster can be summarized in the following steps:\n",
    "1.  **Application(job) Submission**:\n",
    "The user submits a Spark application (job or driver program) to the cluster manager, which is responsible for scheduling\n",
    "the job and allocating resources.\n",
    "2.  **Resource Allocation**:\n",
    "The cluster manager allocates resources (CPU, memory, etc.) to the driver program based on the\n",
    "application's requirements.\n",
    "3.  **Task Scheduling**:\n",
    "The master node schedules tasks based on the application's requirements and the available resources on the worker nodes, dividing the application into smaller tasks that can be executed in parallel.\n",
    "4.  **Task Execution**:\n",
    "The executor processes on the worker nodes execute the tasks processing the data and computing results assigned by the master node.The intermediate data may be cached in memory or written to disk.\n",
    "5.  **Result Collection**:\n",
    "Once the tasks are completed, results are sent back to the driver program, which can then process the output or save it to storage.\n",
    "\n",
    "### Spark Cluster Modes\n",
    "A Spark cluster can operate in several modes, including:\n",
    "\n",
    "1. **Standalone Mode**: A simple cluster mode that does not require any external cluster manager. Spark manages the cluster itself.\n",
    "\n",
    "2. **YARN Mode**: Uses Hadoop YARN for resource management. Spark runs as an application on YARN, utilizing its capabilities for resource allocation and scheduling.\n",
    "\n",
    "3. **Mesos Mode**: Leverages Apache Mesos for managing cluster resources. Spark can share resources dynamically with other frameworks running on Mesos.\n",
    "\n",
    "4. **Kubernetes Mode**: Spark can also run on Kubernetes, allowing it to utilize the container orchestration capabilities of Kubernetes for resource management.\n",
    "\n",
    "### Benefits of Using a Spark Cluster\n",
    "- **Scalability**: A Spark cluster can easily scale horizontally by adding more nodes to handle larger datasets and workloads.\n",
    "- **Fault Tolerance**: The cluster architecture provides resilience to failures, allowing tasks to be retried or rescheduled in case of node failures.\n",
    "- **Parallel Processing**: The distributed nature of the cluster enables Spark to perform computations in parallel, significantly improving performance for large-scale data processing.\n",
    "\n",
    "\n",
    "A Spark cluster is a powerful architecture that enables distributed data processing across multiple nodes, leveraging parallelism and resource management for efficient computation. It is designed to handle large datasets and complex analytics workloads, making Spark a popular choice for big data applications."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
