{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6edce08-10c1-4d34-b835-00aa8e9d4d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/07 15:32:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Retail Sales Analysis\") \\\n",
    "    .getOrCreate()\n",
    "file_path = \"sales.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d123f82-987b-4125-a897-95c20bcf5e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc2b937f-65b8-4f1e-95bb-baa078d5f184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(TransactionID,IntegerType,true),StructField(Date,StringType,true),StructField(StoreID,StringType,true),StructField(ProductID,IntegerType,true),StructField(ProductName,StringType,true),StructField(Quantity,IntegerType,true),StructField(Price,DoubleType,true),StructField(TotalSale,DoubleType,true),StructField(CustomerID,StringType,true)))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f0dadbd-f382-410f-82f3-5eba3a2c15fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-------+---------+-----------+--------+-----+---------+----------+\n",
      "|TransactionID|      Date|StoreID|ProductID|ProductName|Quantity|Price|TotalSale|CustomerID|\n",
      "+-------------+----------+-------+---------+-----------+--------+-----+---------+----------+\n",
      "|            1|2025-01-01|   S001|      101|      Apple|       3| 0.99|     2.97|      C001|\n",
      "|            2|2025-01-02|   S002|      102|     Banana|       2| 0.59|     1.18|      C002|\n",
      "|            3|2025-01-03|   S001|      103|     Orange|       5| 1.29|     6.45|      C003|\n",
      "|            4|2025-01-03|   S003|      104|      Mango|       1| 1.99|     1.99|      C004|\n",
      "|            5|2025-01-04|   S002|      105|  Blueberry|      10| 2.49|     24.9|      C005|\n",
      "+-------------+----------+-------+---------+-----------+--------+-----+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e96f37-a487-423d-9936-bdffaf85c1fd",
   "metadata": {},
   "source": [
    "Spark uses a sample (controlled by samplingRatio) of the data to infer types, typically requiring two reads: one for schema inference and another for loading. The size of the sample depends on samplingRatio, but default might be 100% (all rows), which is bad for big data. Hence, explicit schemas are better for performance.\n",
    "Best Practice\n",
    "1. Explicit Schema for Production Jobs\n",
    "2. Adjust 'samplingRatio'for Large Data\n",
    "3. Validate Inferred Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345276e2-97bb-4c77-8439-148edd077526",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b483480-2a3a-45c7-934f-a0b8c8f2069f",
   "metadata": {},
   "source": [
    "# schema on write \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcb879e9-4220-407b-b37e-d3e8fbd8bed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/08 11:29:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TransactionID: integer (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- StoreID: string (nullable = true)\n",
      " |-- ProductID: string (nullable = true)\n",
      " |-- ProductName: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- TotalSale: double (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Retail Sales Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "file_path = \"sales.csv\"\n",
    "\n",
    "# Define explicit schema\n",
    "custom_schema = StructType([\n",
    "    StructField(\"TransactionID\", IntegerType()),\n",
    "    StructField(\"Date\", DateType()),\n",
    "    StructField(\"StoreID\", StringType()),\n",
    "    StructField(\"ProductID\", StringType()),\n",
    "    StructField(\"ProductName\", StringType()),\n",
    "    StructField(\"Quantity\", IntegerType()),\n",
    "    StructField(\"Price\", DoubleType()),\n",
    "    StructField(\"TotalSale\", DoubleType()),\n",
    "    StructField(\"CustomerID\", StringType())\n",
    "])\n",
    "\n",
    "df = spark.read.csv(file_path, header=True, schema=custom_schema)\n",
    "df.printSchema()  # Verify schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2c45f32-7f1d-429f-bbe8-df9d1627359e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-------+---------+-----------+--------+-----+---------+----------+\n",
      "|TransactionID|      Date|StoreID|ProductID|ProductName|Quantity|Price|TotalSale|CustomerID|\n",
      "+-------------+----------+-------+---------+-----------+--------+-----+---------+----------+\n",
      "|            1|2025-01-01|   S001|      101|      Apple|       3| 0.99|     2.97|      C001|\n",
      "|            2|2025-01-02|   S002|      102|     Banana|       2| 0.59|     1.18|      C002|\n",
      "|            3|2025-01-03|   S001|      103|     Orange|       5| 1.29|     6.45|      C003|\n",
      "|            4|2025-01-03|   S003|      104|      Mango|       1| 1.99|     1.99|      C004|\n",
      "|            5|2025-01-04|   S002|      105|  Blueberry|      10| 2.49|     24.9|      C005|\n",
      "+-------------+----------+-------+---------+-----------+--------+-----+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fa03b7-5f2a-460e-82f6-eb890d6c51ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb37e63c-be82-4d4a-b597-837eb55654ac",
   "metadata": {},
   "source": [
    "Steps performed internally\n",
    "- Schema Binding: binds custom_schema to the data source.(parsing explicit data type)\n",
    "- Data Parsing: Each row is parsed and cast to the schema’s types.\n",
    "- InternalRow Creation: Data is stored in Tungsten’s binary format.\n",
    "    For a CSV row: \"123,2023-10-01,STR001,PROD045,Laptop,2,899.99,1799.98,CUST1001\"\n",
    "        Step 1: The CSV reader splits the row into individual fields based on delimiters.\n",
    "        Step 2: Each field is converted to the specified data type in the schema:\n",
    "            \"123\" → IntegerType (parsed as 123).\n",
    "            \"2023-10-01\" → DateType (parsed as a java.sql.Date object).\n",
    "            \"STR001\" → StringType (preserved as-is).\n",
    "            \"899.99\" → DoubleType (parsed as 899.99).\n",
    "        Step 3: The parsed data is stored as an InternalRow object (Spark’s binary-optimized in-memory format).\n",
    "- Logical Plan: Catalyst builds a plan using the schema for optimizations.\n",
    "    Predicate Pushdown: Filters (e.g., Quantity > 0) are pushed to the data source, reducing I/O.\n",
    "    Column Pruning: Only required columns (e.g., TransactionID, TotalSale) are read from disk.\n",
    "    Type-Specific Optimizations: Operations like arithmetic (TotalSale = Quantity * Price) use native types (e.g., DoubleType), avoiding runtime casting.\n",
    "- Memory and Storage Efficiency:\n",
    "  Data is stored in a compact binary format optimized for the explicit schema.\n",
    "    Example: IntegerType uses 4 bytes per value, while inferred StringType would use variable-length encoding.\n",
    "    No Type Guessing: Eliminates overhead from runtime type checks/casting\n",
    "- Error Handling and Data Validation\n",
    "    Spark enforces the schema during the initial read:\n",
    "    Mode-Specific Handling (set via mode option in spark.read.csv):\n",
    "        PERMISSIVE (default): Invalid records (e.g., non-integer TransactionID) are converted to null and logged.\n",
    "        DROPMALFORMED: Discards invalid rows.\n",
    "        FAILFAST: Throws an error on the first invalid record.\n",
    "        Early Failure: Schema mismatches are caught during job execution (unlike schema inference, where errors may surface later)\n",
    "- Avoid Double Data Read\n",
    "    Unlike inferSchema=True (which reads data twice), explicit schemas require only one pass over the data:\n",
    "    Single Read: Data is parsed and converted to the specified types in a single pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ddd206-084e-413a-bfbe-f700ee8bee7f",
   "metadata": {},
   "source": [
    "# Schema-on-Read vs Schema-on-Write: Advantages and Disadvantages\n",
    "\n",
    "## Schema-on-Read\n",
    "\n",
    "### **Definition**\n",
    "Schema-on-Read is a data processing approach where the schema is applied dynamically when the data is read. The schema is inferred or defined at query time, allowing flexibility in handling diverse data formats.\n",
    "\n",
    "### **Advantages**\n",
    "1. **Flexibility**:\n",
    "   - Can handle semi-structured or unstructured data (e.g., JSON, CSV, Parquet).\n",
    "   - No need to predefine the schema before ingesting data.\n",
    "2. **Rapid Data Ingestion**:\n",
    "   - Data can be ingested quickly without upfront schema design.\n",
    "   - Ideal for exploratory analysis or prototyping.\n",
    "3. **Schema Evolution**:\n",
    "   - Easily adapts to changes in data structure (e.g., new columns or fields).\n",
    "4. **Cost-Effective**:\n",
    "   - Reduces the time and effort required for schema design and maintenance.\n",
    "\n",
    "### **Disadvantages**\n",
    "1. **Performance Overhead**:\n",
    "   - Schema inference at read time can be slow, especially for large datasets.\n",
    "   - May require reading data twice (e.g., sampling for schema inference).\n",
    "2. **Data Quality Issues**:\n",
    "   - Risk of incorrect type inference (e.g., dates inferred as strings).\n",
    "   - No strict validation during ingestion, leading to potential errors downstream.\n",
    "3. **Complexity in Querying**:\n",
    "   - Queries may fail if the inferred schema doesn’t match the actual data.\n",
    "   - Requires additional checks and transformations in queries.\n",
    "4. **Resource Intensive**:\n",
    "   - Sampling and inference can be resource-heavy for large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## Schema-on-Write\n",
    "\n",
    "### **Definition**\n",
    "Schema-on-Write is a traditional approach where the schema is defined and enforced when data is written (ingested) into the system. The data must conform to the predefined schema.\n",
    "\n",
    "### **Advantages**\n",
    "1. **Data Consistency**:\n",
    "   - Ensures data adheres to a predefined structure, improving data quality.\n",
    "   - Strict validation during ingestion prevents invalid data from entering the system.\n",
    "2. **Performance**:\n",
    "   - No runtime schema inference, leading to faster query execution.\n",
    "   - Optimized storage and memory usage due to predefined types.\n",
    "3. **Query Optimization**:\n",
    "   - Enables advanced optimizations like predicate pushdown and column pruning.\n",
    "   - Better performance for structured queries.\n",
    "4. **Error Detection**:\n",
    "   - Errors are caught during ingestion, reducing the risk of downstream failures.\n",
    "\n",
    "### **Disadvantages**\n",
    "1. **Rigidity**:\n",
    "   - Schema changes require updates to the ingestion pipeline.\n",
    "   - Not suitable for rapidly evolving data structures.\n",
    "2. **Upfront Effort**:\n",
    "   - Requires time and effort to design and maintain schemas.\n",
    "   - May delay data ingestion and analysis.\n",
    "3. **Less Flexible**:\n",
    "   - Struggles with semi-structured or unstructured data.\n",
    "   - Not ideal for exploratory analysis or ad-hoc use cases.\n",
    "4. **Cost of Schema Evolution**:\n",
    "   - Migrating data to a new schema can be complex and time-consuming.\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison Table\n",
    "\n",
    "| **Aspect**               | **Schema-on-Read**                                  | **Schema-on-Write**                          |\n",
    "|--------------------------|-----------------------------------------------------|----------------------------------------------|\n",
    "| **Flexibility**          | High (handles diverse data formats).                | Low (requires predefined schema).            |\n",
    "| **Performance**          | Slower (runtime schema inference).                 | Faster (no runtime inference).               |\n",
    "| **Data Quality**         | Lower (risk of incorrect inference).                | Higher (strict validation during ingestion). |\n",
    "| **Schema Evolution**     | Easier (adapts to changes dynamically).             | Harder (requires schema updates).            |\n",
    "| **Use Case**             | Exploratory analysis, prototyping.                 | Production systems, structured data.         |\n",
    "| **Resource Usage**       | Higher (sampling and inference overhead).          | Lower (optimized for predefined schema).     |\n",
    "\n",
    "---\n",
    "\n",
    "## When to Use Which?\n",
    "\n",
    "### **Schema-on-Read**\n",
    "- Use for:\n",
    "  - Exploratory data analysis.\n",
    "  - Prototyping or ad-hoc queries.\n",
    "  - Semi-structured or unstructured data (e.g., JSON, CSV).\n",
    "- Avoid for:\n",
    "  - Production systems with strict data quality requirements.\n",
    "  - Large datasets where performance is critical.\n",
    "\n",
    "### **Schema-on-Write**\n",
    "- Use for:\n",
    "  - Production systems with structured data.\n",
    "  - Scenarios requiring high data quality and consistency.\n",
    "  - Large datasets where performance is critical.\n",
    "- Avoid for:\n",
    "  - Rapidly evolving data structures.\n",
    "  - Exploratory analysis or prototyping.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "- **Schema-on-Read** offers flexibility and rapid ingestion but at the cost of performance and data quality.\n",
    "- **Schema-on-Write** ensures data consistency and performance but requires upfront effort and is less flexible.\n",
    "- Choose the approach based on your use case: **Schema-on-Read** for exploration and **Schema-on-Write** for production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae8ec9-ee4d-4fdd-b2fa-762c14e701e1",
   "metadata": {},
   "source": [
    "# Spark Schema Handling: Schema-on-Read vs Explicit Schema\n",
    "\n",
    "## How Spark Parses and Optimizes Data\n",
    "\n",
    "### **Schema-on-Read (`inferSchema=True`)**  \n",
    "#### Parsing Behavior  \n",
    "- **Dynamic Parsing**:  \n",
    "  - Schema is inferred by sampling the data (default: all rows).  \n",
    "  - Raw data (e.g., CSV strings) is parsed into Spark types (e.g., `\"123\"` → `IntegerType`).  \n",
    "- **Two Data Reads**:  \n",
    "  1. **Sampling Pass**: Reads a subset of data to infer column types.  \n",
    "  2. **Full Read**: Reloads the entire dataset with the inferred schema.  \n",
    "\n",
    "#### Optimization  \n",
    "- **Catalyst Optimizer**:  \n",
    "  - Uses the inferred schema for optimizations like predicate pushdown and column pruning.  \n",
    "  - Less effective if inferred types are incorrect (e.g., `StringType` instead of `DateType`).  \n",
    "- **Tungsten Binary Format**:  \n",
    "  - Parsed data is stored in Spark’s optimized in-memory format (similar to explicit schemas).  \n",
    "\n",
    "#### Key Limitations  \n",
    "- **Performance Overhead**: Double data read (sampling + parsing).  \n",
    "- **Data Quality Risks**: Incorrect type inference (e.g., numeric IDs parsed as `IntegerType` and losing leading zeros).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Explicit Schema Definition**  \n",
    "#### Parsing Behavior  \n",
    "- **Strict Parsing**:  \n",
    "  - Data is parsed directly into user-defined types (e.g., `DateType` for a column).  \n",
    "  - Invalid values (e.g., `\"Oct-2023\"` for `DateType`) are flagged as errors or `null`.  \n",
    "- **Single Data Read**: No sampling overhead.  \n",
    "\n",
    "#### Optimization  \n",
    "- **Catalyst Optimizer**:  \n",
    "  - Leverages explicit types for aggressive optimizations:  \n",
    "    - Predicate pushdown (filters applied at the data source).  \n",
    "    - Column pruning (only required columns are read).  \n",
    "- **Tungsten Efficiency**:  \n",
    "  - Data stored in memory using type-specific encodings (e.g., `IntegerType` uses 4 bytes).  \n",
    "\n",
    "#### Key Advantages  \n",
    "- **Performance**: Single read pass, no sampling overhead.  \n",
    "- **Data Correctness**: Strict validation during ingestion.  \n",
    "\n",
    "---\n",
    "\n",
    "## Internal Workflow Comparison  \n",
    "\n",
    "### **Schema-on-Read Example**  \n",
    "```python\n",
    "df = spark.read.csv(\"sales.csv\", header=True, inferSchema=True)\n",
    "df.filter(df.Price > 100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0b428c-25f7-4628-88cc-83a702350ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
